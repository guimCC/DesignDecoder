{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_features_and_labels(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    features = dataframe.iloc[:, :-11]\n",
    "    labels = dataframe.iloc[:, -11:]\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load product data\n",
    "products = pd.read_csv('data/product_with_attributes.csv')\n",
    "features, labels = split_features_and_labels(products)\n",
    "\n",
    "# Load image embeddings\n",
    "embeddings_file = \"image_embeddings.pt\"\n",
    "embeddings_dict = torch.load(embeddings_file)\n",
    "embeddings = [embeddings_dict[filename] for filename in products['des_filename']]\n",
    "embeddings_array = np.array(embeddings)\n",
    "embedding_df = pd.DataFrame(embeddings_array, columns=[f'embedding_dim_{i}' for i in range(embeddings_array.shape[1])])\n",
    "\n",
    "features = pd.concat([features, embedding_df], axis=1)\n",
    "\n",
    "# Drop the columns that are not needed\n",
    "features = features.drop('cod_modelo_color', axis=1)\n",
    "features = features.drop('des_filename', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label encoding - Store the encodings for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder_dict = {}\n",
    "encoded_labels = pd.DataFrame()\n",
    "for label in labels.columns:\n",
    "    label_encoder = LabelEncoder()\n",
    "    encoded_labels[label] = label_encoder.fit_transform(labels[label])\n",
    "    label_encoder_dict[label] = label_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn to a np.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_arr = np.array(features)\n",
    "features_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_encoder_dict = dict()\n",
    "for feature_idx in range(0, 9):\n",
    "    le = LabelEncoder()\n",
    "    features_arr[:, feature_idx] = le.fit_transform(features_arr[:, feature_idx])\n",
    "    feature_encoder_dict[feature_idx] = le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filtering invalid types:**\n",
    "- Set $\\texttt{FILTER\\_INVALID\\_TRAIN} = \\texttt{TRUE}$ if invalid types are to be filtered before **training**.\n",
    "- Let $\\texttt{FILTER\\_INVALID\\_TRAIN} = \\texttt{FALSE}$ to just filter at **predict** time.\n",
    "\n",
    "**Training with all data:**\n",
    "- Set $\\texttt{TRAIN\\_ALL\\_DATA} = \\texttt{TRUE}$ to push training further and try to achieve beter metrics at **test** dataset.\n",
    "- Let $\\texttt{TRAIN\\_ALL\\_DATA} = \\texttt{FALSE}$ to split a **validation** dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER_INVALID_TRAIN = True\n",
    "TRAIN_ALL_DATA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the dataset for each attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dict = dict()\n",
    "y_train_dict = dict()\n",
    "X_val_dict = dict()\n",
    "y_val_dict = dict()\n",
    "\n",
    "invalid_types = torch.load('invalid_types.pth')\n",
    "for label in labels.columns:\n",
    "    # Retrieve the original categorical values\n",
    "    transformed_features = feature_encoder_dict[7].inverse_transform(features_arr[:, 7].astype(int))\n",
    "\n",
    "    # Create a mask to filter out the invalid types - Check if the transformed features are in the invalid types\n",
    "    mask = np.isin(transformed_features, invalid_types[label])\n",
    "\n",
    "    if FILTER_INVALID_TRAIN:\n",
    "        filtered_features = features_arr[~mask]\n",
    "        filtered_encoded_labels = encoded_labels[label][~mask]\n",
    "    else:\n",
    "        filtered_features = features_arr\n",
    "        filtered_encoded_labels = encoded_labels[label]\n",
    "\n",
    "    if TRAIN_ALL_DATA:    \n",
    "        X_train_dict[label], X_val_dict[label], y_train_dict[label], y_val_dict[label] = train_test_split(filtered_features, filtered_encoded_labels, test_size=0.1, random_state=42)\n",
    "    else:\n",
    "        X_train_dict[label] = filtered_features\n",
    "        y_train_dict[label] = filtered_encoded_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a $\\texttt{XBGClassifier}$ model for each attribute. After lots of hyperparameter tunning, this config performed the best:\n",
    "- $\\texttt{eval\\_metric = 'mlogloss'}$\n",
    "- $\\texttt{max\\_depth = 8}$\n",
    "- $\\texttt{learning\\_rate = 0.1}$\n",
    "- $\\texttt{n\\_estimators = 400}$\n",
    "- $\\texttt{subsample = 0.8}$\n",
    "- $\\texttt{colsample\\_bytree = 0.8}$\n",
    "- $\\texttt{gamma = 0.1}$\n",
    "- $\\texttt{min\\_child\\_weight = 5}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in labels:\n",
    "    # Define the classifier model fel each attribute\n",
    "    clf_model = XGBClassifier(\n",
    "        eval_metric='mlogloss',\n",
    "        max_depth=8,\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=400,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        gamma=0.1,\n",
    "        min_child_weight=5,\n",
    "        \n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Fit the model\n",
    "    clf_model.fit(X_train_dict[label], y_train_dict[label])\n",
    "\n",
    "    # Save the model\n",
    "    model_filename = f\"model_{label}_xgb.joblib\"\n",
    "    joblib.dump(clf_model, model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/media/guimcc/Elements/datathon\"\n",
    "CSV_PATH = f\"{PATH}/archive/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the test embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(f'{CSV_PATH}test_data.csv')\n",
    "embeddings_test = torch.load('image_embeddings_test.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the already trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {label: joblib.load(f\"model_{label}_xgb.joblib\") for label in labels.columns}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some initial declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['des_sex', 'des_age',\t'des_line',\t'des_fabric', 'des_product_category', 'des_product_aggregated_family', 'des_product_family', 'des_product_type', 'des_color']\n",
    "predictions = pd.DataFrame()\n",
    "predictions['test_id'] = test['test_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the predictions using the corresponding model for each attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_count = 0\n",
    "\n",
    "for index, row in tqdm(test.iterrows()):\n",
    "    # Retrieve the product id and the attribute\n",
    "    splitted = row['test_id'].split('_')\n",
    "    product_id = '_'.join(splitted[:2])\n",
    "    attribute = '_'.join(splitted[2:])\n",
    "\n",
    "    # If the product type is invalid, set the prediction to INVALID and skip prediction\n",
    "    if test.loc[index]['des_product_type'] in invalid_types[attribute]:\n",
    "        predictions.loc[index, 'des_value'] = 'INVALID'\n",
    "        invalid_count += 1\n",
    "        continue\n",
    "    \n",
    "    # Encode the test features\n",
    "    encoded_features = np.zeros((1, 9))\n",
    "    for feature_idx in range(0, 9):\n",
    "        le = feature_encoder_dict[feature_idx]\n",
    "        try:\n",
    "            encoded_features[0][feature_idx] = le.transform([test.loc[index, col_names[feature_idx]]])[0]\n",
    "        except ValueError:\n",
    "            encoded_features[0][feature_idx] = np.nan\n",
    "        \n",
    "    \n",
    "    # Get the embedding for the product\n",
    "    key = test.loc[index]['des_filename']\n",
    "    model = models[attribute]\n",
    "    embedding = embeddings_test[key].numpy()\n",
    "    embedding = embedding.reshape(1, -1)\n",
    "    \n",
    "    # Concatenate the encoded features with the embedding\n",
    "    input_features = np.concatenate((encoded_features, embedding), axis=1)\n",
    "    \n",
    "    # Make the prediction\n",
    "    prediction = model.predict(input_features)\n",
    "    predictions.loc[index, 'des_value'] = label_encoder_dict[attribute].inverse_transform(prediction) #unique_values[attribute][prediction[0]]\n",
    "\n",
    "print(f\"Number of invalid products: {invalid_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the predictions for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv('data/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
